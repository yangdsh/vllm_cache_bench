# GPU Cache Experiment Configuration
# Comprehensive cache pressure study with conversational vs non-conversational workloads

tag: "llama-8b"
log_base_dir: "outputs/logs/experiments"
model_name: "llama-8b"

defaults:
  num_prompts: 30000
  time_limit: 1200
  use_conversation_evictions: [true, false]
  mock_decoding: false
  # Global conversation eviction config (used if not specified per dataset)
  conversation_eviction_config:
    reference_recency: 100.0
    reference_time_interval: 100.0
    score_factor: 3.0

experiments:
    datasets:
      - path: "data/cw_logs_5_29_5am_6am.csv"
        cache_sizes: [32, 64, 128, 256]
        request_rates: [0.9]